# ğŸ˜ˆ SalmAlm v0.9.2

**Personal AI Gateway â€” Pure Python, Zero Dependencies**

> [ğŸ‡°ğŸ‡· í•œêµ­ì–´](README.md)

A self-hosted AI gateway built entirely on Python's standard library. No npm, no dependency hell, no Docker required. One command and you're running your own AI assistant with 30 tools.

The only optional dependency is `cryptography` for AES-256-GCM vault encryption. Without it, the vault falls back to HMAC-CTR (still secure, just not AEAD).

## ğŸš€ Quick Start

### pip (Recommended)

```bash
pip install salmalm
salmalm
# â†’ Opens http://localhost:18800
# Add your API keys in Settings
```

### Docker

```bash
docker run -p 18800:18800 -p 18801:18801 \
  -e SALMALM_VAULT_PW=changeme \
  -v salmalm_data:/app \
  $(docker build -q .)
```

### Docker Compose

```bash
git clone https://github.com/hyunjun6928-netizen/salmalm.git
cd salmalm
# Edit docker-compose.yml â€” set SALMALM_VAULT_PW and API keys
docker compose up -d
# â†’ http://localhost:18800
```

### Local LLM (Ollama â€” no API key needed)

```bash
ollama pull llama3.2
salmalm
# In Settings, enter Ollama URL: http://localhost:11434/v1
# Use: /model ollama/llama3.2
```

## âœ¨ What It Does

Think of it as your own local ChatGPT, but with superpowers:

- **Talk to 27 LLMs** â€” Claude, GPT, Grok, Gemini, DeepSeek, Llama â€” through one interface
- **30 built-in tools** â€” run commands, edit files, search the web, manage cron jobs, control browsers
- **RAG search** â€” BM25 over your local files, no OpenAI embeddings needed
- **MCP support** â€” connect to Cursor, VS Code, or any MCP-compatible client
- **WebSocket** â€” real-time streaming via a from-scratch RFC 6455 implementation
- **Telegram & Discord bots** â€” chat from your phone
- **Plugin system** â€” drop a `.py` file in `plugins/` and it just works
- **One-click update** â€” upgrade from Settings UI

## ğŸ—ï¸ Architecture (20 Modules, ~9,000 lines)

```
salmalm/
â”œâ”€â”€ constants.py      â€” config, costs, thresholds
â”œâ”€â”€ crypto.py         â€” AES-256-GCM vault (HMAC-CTR fallback)
â”œâ”€â”€ core.py           â€” audit, cache, sessions, cron, routing
â”œâ”€â”€ llm.py            â€” multi-provider LLM calls
â”œâ”€â”€ tools.py          â€” 30 tool definitions + executor
â”œâ”€â”€ prompt.py         â€” system prompt builder
â”œâ”€â”€ engine.py         â€” Intelligence Engine (classify â†’ plan â†’ execute â†’ reflect)
â”œâ”€â”€ telegram.py       â€” async Telegram bot
â”œâ”€â”€ discord_bot.py    â€” Discord Gateway + HTTP API
â”œâ”€â”€ web.py            â€” Web UI + REST API + SSE streaming
â”œâ”€â”€ ws.py             â€” WebSocket server (RFC 6455)
â”œâ”€â”€ rag.py            â€” BM25 search engine (SQLite-backed)
â”œâ”€â”€ mcp.py            â€” Model Context Protocol server + client
â”œâ”€â”€ browser.py        â€” Chrome DevTools Protocol automation
â”œâ”€â”€ nodes.py          â€” SSH/HTTP remote node control
â”œâ”€â”€ stability.py      â€” circuit breaker, health monitor, watchdog
â”œâ”€â”€ auth.py           â€” JWT auth, RBAC, rate limiter, PBKDF2
â”œâ”€â”€ tls.py            â€” self-signed TLS cert generation
â”œâ”€â”€ logging_ext.py    â€” JSON structured logging, rotation
â”œâ”€â”€ container.py      â€” lightweight DI container
â””â”€â”€ docs.py           â€” auto-generated API documentation
```

## ğŸ” Security

| Layer | Implementation |
|-------|---------------|
| **Vault** | AES-256-GCM (or HMAC-CTR fallback), PBKDF2 200K iterations |
| **Auth** | JWT tokens (HMAC-SHA256), API keys, PBKDF2 password hashing |
| **RBAC** | admin / user / readonly roles with permission matrix |
| **CORS** | Same-origin whitelist only (127.0.0.1, localhost) |
| **Rate Limit** | Token bucket per user + per IP, configurable per role |
| **Upload** | Filename sanitization, 50MB limit, path traversal prevention |
| **Exec** | Command blocklist + regex pattern matching + subprocess isolation |
| **Audit** | SHA-256 hash chain, tamper-evident log |

## ğŸ“¡ API

| Endpoint | Auth | Description |
|----------|------|-------------|
| `GET /api/status` | No | Version, usage stats |
| `GET /api/health` | No | Component health (8 checks) |
| `POST /api/auth/login` | No | Get JWT token |
| `POST /api/unlock` | No | Unlock vault |
| `POST /api/chat` | Yes | Send message |
| `POST /api/chat/stream` | Yes | SSE streaming response |
| `POST /api/vault` | Admin | Key CRUD |
| `GET /api/dashboard` | Yes | Sessions, costs, cron |
| `GET /api/rag/search?q=...` | Yes | BM25 search |
| `GET /docs` | No | Auto-generated API docs |
| `ws://127.0.0.1:18801` | â€” | WebSocket |

## ğŸ”‘ Supported Models

| Provider | Models |
|----------|--------|
| Anthropic | Claude Opus 4, Sonnet 4, Haiku 3.5 |
| OpenAI | GPT-5.3 Codex, GPT-4.1, o3, o4-mini |
| xAI | Grok-4, Grok-3, Grok-3 Mini |
| Google | Gemini 3 Pro/Flash, Gemini 2.5 Pro/Flash |
| DeepSeek | R1, Chat |
| Meta | Llama 4 Maverick, Scout |

All with per-model cost tracking (input + output tokens â†’ USD).

## ğŸ§  Intelligence Engine

Not just a chat proxy. Every message goes through:

1. **Intent Classification** â€” 7 tiers (trivial â†’ emergency) with keyword + length analysis
2. **Model Selection** â€” cheapest model that can handle the task
3. **Tool Planning** â€” which tools to call, in what order
4. **Parallel Execution** â€” independent tool calls run concurrently
5. **Self-Reflection** â€” checks output quality, retries if needed

## ğŸ“Š Stats

- ~9,000 lines of Python across 20 modules
- 30 built-in tools + plugin extensibility
- 27 LLM models with cost tracking
- 85 unit tests
- 18/18 self-test on startup
- 8-component health monitoring

## ğŸ“œ License

MIT
