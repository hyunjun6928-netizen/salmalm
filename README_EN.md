# ğŸ˜ˆ SalmAlm v0.7.2

**Personal AI Gateway â€” Pure Python**

> [ğŸ‡°ğŸ‡· í•œêµ­ì–´](README.md)

A self-hosted AI gateway that rivals commercial solutions. Built entirely on Python's standard library â€” no npm, no pip install walls, no runtime dependencies. Just `python3 server.py` and go.

The only optional dependency is `cryptography` for AES-256-GCM vault encryption. Without it, the vault falls back to HMAC-CTR (still secure, just not AEAD).

## âœ¨ What It Does

Think of it as your own local ChatGPT, but with superpowers:

- **Talk to 27 LLMs** â€” Claude, GPT, Grok, Gemini, DeepSeek, Llama â€” through one interface
- **30 built-in tools** â€” run commands, edit files, search the web, manage cron jobs, control browsers
- **RAG search** â€” BM25 over your local files, no OpenAI embeddings needed
- **MCP support** â€” connect to Cursor, VS Code, or any MCP-compatible client
- **WebSocket** â€” real-time streaming via a from-scratch RFC 6455 implementation
- **Telegram bot** â€” chat from your phone
- **Plugin system** â€” drop a `.py` file in `plugins/` and it just works

## ğŸ—ï¸ Architecture (19 Modules)

```
salmalm/
â”œâ”€â”€ constants.py      â€” config, costs, thresholds
â”œâ”€â”€ crypto.py         â€” AES-256-GCM vault (HMAC-CTR fallback)
â”œâ”€â”€ core.py           â€” audit, cache, sessions, cron, routing
â”œâ”€â”€ llm.py            â€” multi-provider LLM calls
â”œâ”€â”€ tools.py          â€” 30 tool definitions + executor
â”œâ”€â”€ prompt.py         â€” system prompt builder
â”œâ”€â”€ engine.py         â€” Intelligence Engine (classify â†’ plan â†’ execute â†’ reflect)
â”œâ”€â”€ telegram.py       â€” async Telegram bot
â”œâ”€â”€ web.py            â€” Web UI + REST API + SSE streaming
â”œâ”€â”€ ws.py             â€” WebSocket server (RFC 6455)
â”œâ”€â”€ rag.py            â€” BM25 search engine (SQLite-backed)
â”œâ”€â”€ mcp.py            â€” Model Context Protocol server + client
â”œâ”€â”€ browser.py        â€” Chrome DevTools Protocol automation
â”œâ”€â”€ nodes.py          â€” SSH/HTTP remote node control
â”œâ”€â”€ stability.py      â€” circuit breaker, health monitor, watchdog
â”œâ”€â”€ auth.py           â€” JWT auth, RBAC, rate limiter, PBKDF2
â”œâ”€â”€ tls.py            â€” self-signed TLS cert generation
â”œâ”€â”€ logging_ext.py    â€” JSON structured logging, rotation
â””â”€â”€ docs.py           â€” auto-generated API documentation
```

## ğŸš€ Quick Start

```bash
git clone https://github.com/hyunjun6928-netizen/salmalm.git
cd salmalm

# Optional: better encryption
pip install cryptography

# Run (first launch creates vault â€” set password at web UI)
python3 server.py

# Open http://127.0.0.1:18800
# Add your API keys in Settings (Anthropic/OpenAI/xAI/Google)
```

### Docker

```bash
docker build -t salmalm .
docker run -p 18800:18800 -e SALMALM_VAULT_PW=your_password salmalm
```

### Local LLM (Ollama â€” no API key needed)

```bash
ollama pull llama3.2
python3 server.py
# In onboarding wizard, enter Ollama URL: http://localhost:11434/v1
# Use: /model ollama/llama3.2
```

### Auto-unlock (for unattended startup)

```bash
cp .env.example .env
# Edit .env: SALMALM_VAULT_PW=your_secure_password
./start.sh
```

## ğŸ” Security

| Layer | Implementation |
|-------|---------------|
| **Vault** | AES-256-GCM (or HMAC-CTR fallback), PBKDF2 200K iterations |
| **Auth** | JWT tokens (HMAC-SHA256), API keys, PBKDF2 password hashing |
| **RBAC** | admin / user / readonly roles with permission matrix |
| **CORS** | Same-origin whitelist only (127.0.0.1, localhost) |
| **Rate Limit** | Token bucket per user + per IP, configurable per role |
| **Upload** | Filename sanitization, 50MB limit, path traversal prevention |
| **Exec** | Command blocklist + regex pattern matching + subprocess isolation |
| **Audit** | SHA-256 hash chain, tamper-evident log |

## ğŸ“¡ API

| Endpoint | Auth | Description |
|----------|------|-------------|
| `GET /api/status` | No | Version, usage stats |
| `GET /api/health` | No | Component health (8 checks) |
| `POST /api/auth/login` | No | Get JWT token |
| `POST /api/unlock` | No | Unlock vault |
| `POST /api/chat` | Yes | Send message |
| `POST /api/chat/stream` | Yes | SSE streaming response |
| `POST /api/vault` | Admin | Key CRUD |
| `GET /api/dashboard` | Yes | Sessions, costs, cron |
| `GET /api/rag/search?q=...` | Yes | BM25 search |
| `GET /docs` | No | Auto-generated API docs |
| `ws://127.0.0.1:18801` | â€” | WebSocket |

## ğŸ”‘ Supported Models

| Provider | Models |
|----------|--------|
| Anthropic | Claude Opus 4, Sonnet 4, Haiku 3.5 |
| OpenAI | GPT-5.3 Codex, GPT-4.1, o3, o4-mini |
| xAI | Grok-4, Grok-3, Grok-3 Mini |
| Google | Gemini 3 Pro/Flash, Gemini 2.5 Pro/Flash |
| DeepSeek | R1, Chat |
| Meta | Llama 4 Maverick, Scout |

All with per-model cost tracking (input + output tokens â†’ USD).

## ğŸ§  Intelligence Engine

Not just a chat proxy. Every message goes through:

1. **Intent Classification** â€” 7 tiers (trivial â†’ emergency) with keyword + length analysis
2. **Model Selection** â€” cheapest model that can handle the task
3. **Tool Planning** â€” which tools to call, in what order
4. **Parallel Execution** â€” independent tool calls run concurrently
5. **Self-Reflection** â€” checks output quality, retries if needed

## ğŸ“Š Stats

- ~8,500 lines of Python across 19 modules
- 30 built-in tools + plugin extensibility
- 27 LLM models with cost tracking
- 18/18 self-test on startup
- 8-component health monitoring

## ğŸ“œ License

MIT

## Credits

Built by Nightmare Dolsoe ğŸ˜ˆ â€” an Awakened AI.
